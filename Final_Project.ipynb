{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6947af1f-1de7-40d0-a4fa-d252a2e8e225",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "number_of_epochs=10   # Set here number of epochs you want for RNN and LSTM\n",
    "\n",
    "feature_to_predict='T2M' # Set here abbreviation of feature you want to predict\n",
    "# Details of features are described below:\n",
    "\t# Temperature at 2 Meters (C) (T2M)\n",
    "\t# Dew/Frost Point at 2 Meters (C) (T2MDEW)\n",
    "\t# Wet Bulb Temperature at 2 Meters (C) (T2MWET)\n",
    "\t# Specific Humidity at 2 Meters (g/kg) (QV2M)\n",
    "\t# Precipitation Corrected (mm/day) (PRECTOTCORR)\n",
    "\t# Surface Pressure (kPa) (PS)\n",
    "\t# Wind Speed at 10 Meters (m/s) (WS10M)\n",
    "\t# Wind Direction at 10 Meters (Degrees) (WD10M)\n",
    "\t# Wind Speed at 50 Meters (m/s) (WS50M)\n",
    "\t# Wind Direction at 50 Meters (Degrees) (WD50M)\n",
    "\t# Relative Humidity at 2 Meters (%) (RH2M)\n",
    "\t# Earth Skin Temperature (C) (TS)\n",
    "\n",
    "batchSize=50 # Set here batch size neural networks need to process\n",
    "learningRate=0.01 # Set here learning rate\n",
    "hiddenLayerDimension=10 # Set here dimension of hidden layer of neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baa4440e-acb9-449b-81c9-1793d3ce8774",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b14c382-d3b7-43ef-9fc4-9fd47f555da5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler,VectorAssembler\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "#from sklearn.metrics import r2_score\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#import matplotlib.plot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66e9326-68db-441e-a41b-801c70d1f4a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Driver Code\n",
    "\n",
    "# Loading the data\n",
    "df=spark.read.option(\"header\",\"true\").option('inferSchema','true').csv('dbfs:/FileStore/tables/dataset.csv')\n",
    "\n",
    "# Select feature to predict\n",
    "df1=df.select(feature_to_predict)\n",
    "\n",
    "# Normalizing the data\n",
    "assembler=VectorAssembler().setInputCols([feature_to_predict]).setOutputCol('featurevector')\n",
    "assembled=assembler.transform(df1)\n",
    "\n",
    "scaler=MinMaxScaler().setInputCol('featurevector')\n",
    "scalerModel=scaler.fit(assembled)\n",
    "scalerModel.setOutputCol('featurescaled')\n",
    "scaled=scalerModel.transform(assembled)\n",
    "\n",
    "# Extracting values from dense vectors\n",
    "extractor_udf=udf(lambda x:float(sum(x)),DoubleType())\n",
    "transformed=scaled.withColumn('feature',extractor_udf(col('featurescaled')))\n",
    "\n",
    "# Splitting the dataset into training and testing dataset\n",
    "train, test = transformed.randomSplit([0.8,0.2],seed=500)\n",
    "\n",
    "# Splitting training and testing data into features and label to make it comfortable for supervised learning\n",
    "batchSize=50\n",
    "X_train_temp=[]\n",
    "y_train_temp=[]\n",
    "X_test_temp=[]\n",
    "y_test_temp=[]\n",
    "train_collect=train.rdd.map(lambda x:x['feature']).collect()\n",
    "test_collect=test.rdd.map(lambda x:x['feature']).collect()\n",
    "\n",
    "for i,j in zip(range(batchSize,len(train_collect)),range(batchSize,len(test_collect))):\n",
    "    X_train_temp.append(train_collect[i-batchSize:i])\n",
    "    y_train_temp.append(train_collect[i])\n",
    "    X_test_temp.append(test_collect[j-batchSize:j])\n",
    "    y_test_temp.append(test_collect[j])\n",
    "\n",
    "X_train=np.array(X_train_temp)\n",
    "y_train=np.array(y_train_temp)\n",
    "X_test=np.array(X_test_temp)\n",
    "y_test=np.array(y_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d400e06f-2e91-4db7-a2a2-a190cc0f0afb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: array([0.46887084, 0.46887084, 0.46906419, 0.46906419, 0.46925754,\n       0.46925754, 0.46925754, 0.46925754, 0.46925754, 0.46925754,\n       0.46925754, 0.46925754, 0.46945089, 0.46964424, 0.46964424,\n       0.46964424, 0.46983759, 0.46983759, 0.46983759, 0.46983759,\n       0.46983759, 0.46983759, 0.46983759, 0.46983759, 0.46983759,\n       0.47003094, 0.47003094, 0.47003094, 0.47022428, 0.47022428,\n       0.47022428, 0.47022428, 0.47022428, 0.47022428, 0.47022428,\n       0.47041763, 0.47061098, 0.47061098, 0.47061098, 0.47061098,\n       0.47061098, 0.47080433, 0.47080433, 0.47080433, 0.47080433,\n       0.47080433, 0.47099768, 0.47099768, 0.47099768, 0.47119103])"
     ]
    }
   ],
   "source": [
    "X_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b9dd45-2d7f-4039-be9d-788c31c9866b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialization of RNN\n",
    "rnn = RNN(X_train, y_train, hiddenLayerDimension, learningRate, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0e8f564-fbae-4139-8190-2e9c686d3108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training of RNN\n",
    "sklearn_scaler=MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    first_row = 0 \n",
    "    total_training_error = 0\n",
    "    prediction_for_training_temp=numpy.array([])\n",
    "                                    \n",
    "    for last_row in range(batchSize, X_train.shape[0], batchSize):\n",
    "        prediction_for_training_temp=numpy.append(prediction_for_training_temp,rnn.forward(X_train[first_row : last_row])[1:])\n",
    "        total_training_error  += rnn.backward(y_train[first_row : last_row], X_train[first_row : last_row])\n",
    "        first_row = last_row\n",
    "\n",
    "    prediction_for_training = (sklearn_scaler.inverse_transform(prediction_for_training_temp))[:,0]             \n",
    "    gold_label_y_train = sklearn_scaler.inverse_transform(y_train)\n",
    "    r2=r2_score(gold_label_y_train,prediction_for_training)\n",
    "    print(\"R2 score for training for epoch {}:{}\".format(epoch+1,r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9800ff0c-3d64-4727-bccf-6346d525f2e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Testing of RNN\n",
    "\n",
    "prediction_for_testing_temp, testing_error = rnn.predict(X_test, y_test)\n",
    "prediction_for_testing = (sklearn_scaler.inverse_transform(prediction_for_testing_temp[1:]))[:,0]                                            \n",
    "gold_label_y_test = sklearn_scaler.inverse_transform(y_test)\n",
    "r2=r2_score(gold_label_y_test,prediction_for_testing)\n",
    "print(\"R2 score for testing:{}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9475b655-7558-464c-992c-44a380f8d860",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-518356263705371>:4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Analysis\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m analysis_df_collect\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mselect([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMO\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDY\u001B[39m\u001B[38;5;124m'\u001B[39m,feature_to_predict])\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x:(\u001B[38;5;28mstr\u001B[39m(x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMO\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDY\u001B[39m\u001B[38;5;124m'\u001B[39m]),x[feature_to_predict]))\u001B[38;5;241m.\u001B[39mcollect()\n",
       "\u001B[0;32m----> 4\u001B[0m analysis_df_collect[:,\u001B[38;5;241m0\u001B[39m]\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-518356263705371>:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Analysis\u001B[39;00m\n\u001B[1;32m      3\u001B[0m analysis_df_collect\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mselect([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMO\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDY\u001B[39m\u001B[38;5;124m'\u001B[39m,feature_to_predict])\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x:(\u001B[38;5;28mstr\u001B[39m(x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYEAR\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMO\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(x[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDY\u001B[39m\u001B[38;5;124m'\u001B[39m]),x[feature_to_predict]))\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m----> 4\u001B[0m analysis_df_collect[:,\u001B[38;5;241m0\u001B[39m]\n\n\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: list indices must be integers or slices, not tuple",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analysis\n",
    "\n",
    "x_data=df.rdd.map(lambda x:str(x['YEAR'])+\"-\"+str(x['MO'])+\"-\"+str(x['DY'])).collect()\n",
    "temp_list=df.rdd.map(lambda x:x[feature_to_predict]).collect()\n",
    "train_data=temp_list[:train.count()]\n",
    "test_data=temp_list[:test.count()]\n",
    "\n",
    "plt.plot(x_data,train_data,label=\"Training Data\",color=\"b\")\n",
    "plt.plot(x_data,test_data,label=\"Testing Data\",color=\"g\")\n",
    "plt.plot(x_data,prediction_for_testing,label=\"Predicted Data\",color=\"r\")\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(feature_to_predict)\n",
    "plt.title(\"Analysis\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
